{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "N1UKyl1oypQn",
        "outputId": "9bfce7d6-db6d-4565-bb03-0da870405d94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1. What is Logistic Regression, and how does it differ from Linear Regression?\\nLogistic Regression is a classification algorithm used to predict binary or categorical outcomes.\\n\\nLinear Regression predicts continuous values.\\n\\nLogistic Regression uses the sigmoid function to map predicted values between 0 and 1, representing probabilities.\\n\\n2. What is the mathematical equation of Logistic Regression?\\n𝑃(𝑦=1∣𝑥)=𝜎(𝑧)=11+−𝑧where𝑧=𝑤𝑇𝑥+𝑏\\n\\n3. Why do we use the Sigmoid function in Logistic Regression?\\nThe sigmoid function maps any real-valued number into the (0, 1) interval.\\nIt outputs probabilities, which are ideal for binary classification.\\n\\n4. What is the cost function of Logistic Regression?\\nThe log loss or binary cross-entropy:\\n\\n𝐽(𝑤)=−1𝑚∑𝑖=1𝑚[𝑦(𝑖)log\\u2061(𝑦^(𝑖))+(1−𝑦(𝑖))\\nlog\\u2061(1−𝑦^(𝑖))]\\n\\n5. What is Regularization in Logistic Regression? Why is it needed?\\nRegularization prevents overfitting by penalizing large weights.\\nIt adds a penalty term to the cost function:\\n\\nL1 (Lasso): 𝜆∑∣𝑤𝑖∣\\n\\nL2 (Ridge): 𝜆∑𝑤𝑖2\\n\\n6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\\nLasso (L1): Shrinks some coefficients to zero → performs feature selection.\\n\\nRidge (L2): Shrinks coefficients but keeps all features.\\n\\nElastic Net: Combines L1 + L2, good when features are correlated.\\n\\n7. When should we use Elastic Net instead of Lasso or Ridge?\\nUse Elastic Net when:\\n\\nYou have many correlated features.\\n\\nYou need both feature selection (from Lasso) and stability (from Ridge).\\n\\n8. What is the impact of the regularization parameter (λ) in Logistic Regression?\\nHigher λ: More regularization → smaller weights → less overfitting.\\n\\nLower λ: Less regularization → model may overfit.\\n\\nλ is a hyperparameter and must be tuned.\\n\\n9. What are the key assumptions of Logistic Regression?\\nLinearity of log-odds (not features and output directly).\\n\\nNo or little multicollinearity among features.\\n\\nIndependence of observations.\\n\\nLarge sample size for reliable estimates.\\n\\n10. What are some alternatives to Logistic Regression for classification tasks?\\nDecision Trees\\n\\nRandom Forest\\n\\nSupport Vector Machines (SVM)\\n\\nk-Nearest Neighbors (k-NN)\\n\\nNaive Bayes\\n\\nNeural Networks\\n\\n11. What are Classification Evaluation Metrics?\\nAccuracy\\n\\nPrecision\\n\\nRecall\\n\\nF1 Score\\n\\nROC-AUC\\n\\nConfusion Matrix\\n\\n12. How does class imbalance affect Logistic Regression?\\nIt causes the model to bias toward the majority class.\\n\\nEvaluation metrics like accuracy become misleading.\\n\\nUse techniques like:\\n\\nResampling (oversample minority, undersample majority)\\n\\nClass weights\\n\\nUse precision-recall or F1 score instead of accuracy.\\n\\n13. What is Hyperparameter Tuning in Logistic Regression?\\nIt involves optimizing parameters like:\\n\\nλ (regularization strength)\\n\\nPenalty type (L1, L2)\\n\\nSolver\\n\\nTechniques: Grid Search, Random Search, Cross-validation\\n\\n14. What are different solvers in Logistic Regression? Which one should be used?\\nliblinear: Good for small datasets, supports L1 and L2.\\n\\nsaga: Supports both L1 and Elastic Net, efficient for large datasets.\\n\\nlbfgs: Good for multiclass problems, fast, supports L2.\\n\\nnewton-cg: Efficient for L2-regularization.\\n\\nRecommendation: Use saga for large datasets and lbfgs for multiclass.\\n\\n15. How is Logistic Regression extended for multiclass classification?\\nOne-vs-Rest (OvR): Trains a binary classifier for each class.\\n\\nSoftmax Regression (Multinomial Logistic Regression): Generalizes logistic regression to handle multiple classes directly.\\n\\n16. What are the advantages and disadvantages of Logistic Regression?\\nAdvantages:\\n\\nSimple and interpretable\\n\\nFast to train\\n\\nWorks well with linearly separable data\\n\\nDisadvantages:\\n\\nAssumes linearity in log-odds\\n\\nNot good for complex patterns\\n\\nSensitive to outliers and multicollinearity\\n\\n17. What are some use cases of Logistic Regression?\\nEmail spam detection\\n\\nDisease prediction (e.g., diabetes, cancer)\\n\\nCredit scoring\\n\\nMarketing (likelihood of purchase)\\n\\nCustomer churn prediction\\n\\n18. What is the difference between Softmax Regression and Logistic Regression?\\nLogistic Regression: For binary classification.\\n\\nSoftmax Regression: For multiclass classification, outputs probability distribution over classes using softmax.\\n\\n19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\\nOvR: Simpler, faster, works well if classes are not highly overlapping.\\n\\nSoftmax: More accurate for mutually exclusive classes, better when class dependencies exist.\\n\\n20. How do we interpret coefficients in Logistic Regression?\\nCoefficients represent the change in log-odds of the outcome with a unit change in the predictor.\\n\\nexp\\u2061(𝑤𝑖)\\nexp(wi): Tells how the odds change (e.g., if \\nexp(𝑤𝑖)=2\\nexp(w i)=2, odds double with 1 unit increase in feature 𝑥𝑖).\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"\n",
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "Logistic Regression is a classification algorithm used to predict binary or categorical outcomes.\n",
        "\n",
        "Linear Regression predicts continuous values.\n",
        "\n",
        "Logistic Regression uses the sigmoid function to map predicted values between 0 and 1, representing probabilities.\n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression?\n",
        "𝑃(𝑦=1∣𝑥)=𝜎(𝑧)=11+−𝑧where𝑧=𝑤𝑇𝑥+𝑏\n",
        "\n",
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "The sigmoid function maps any real-valued number into the (0, 1) interval.\n",
        "It outputs probabilities, which are ideal for binary classification.\n",
        "\n",
        "4. What is the cost function of Logistic Regression?\n",
        "The log loss or binary cross-entropy:\n",
        "\n",
        "𝐽(𝑤)=−1𝑚∑𝑖=1𝑚[𝑦(𝑖)log⁡(𝑦^(𝑖))+(1−𝑦(𝑖))\n",
        "log⁡(1−𝑦^(𝑖))]\n",
        "\n",
        "5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "Regularization prevents overfitting by penalizing large weights.\n",
        "It adds a penalty term to the cost function:\n",
        "\n",
        "L1 (Lasso): 𝜆∑∣𝑤𝑖∣\n",
        "\n",
        "L2 (Ridge): 𝜆∑𝑤𝑖2\n",
        "\n",
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "Lasso (L1): Shrinks some coefficients to zero → performs feature selection.\n",
        "\n",
        "Ridge (L2): Shrinks coefficients but keeps all features.\n",
        "\n",
        "Elastic Net: Combines L1 + L2, good when features are correlated.\n",
        "\n",
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "Use Elastic Net when:\n",
        "\n",
        "You have many correlated features.\n",
        "\n",
        "You need both feature selection (from Lasso) and stability (from Ridge).\n",
        "\n",
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "Higher λ: More regularization → smaller weights → less overfitting.\n",
        "\n",
        "Lower λ: Less regularization → model may overfit.\n",
        "\n",
        "λ is a hyperparameter and must be tuned.\n",
        "\n",
        "9. What are the key assumptions of Logistic Regression?\n",
        "Linearity of log-odds (not features and output directly).\n",
        "\n",
        "No or little multicollinearity among features.\n",
        "\n",
        "Independence of observations.\n",
        "\n",
        "Large sample size for reliable estimates.\n",
        "\n",
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "Decision Trees\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Support Vector Machines (SVM)\n",
        "\n",
        "k-Nearest Neighbors (k-NN)\n",
        "\n",
        "Naive Bayes\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "11. What are Classification Evaluation Metrics?\n",
        "Accuracy\n",
        "\n",
        "Precision\n",
        "\n",
        "Recall\n",
        "\n",
        "F1 Score\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "12. How does class imbalance affect Logistic Regression?\n",
        "It causes the model to bias toward the majority class.\n",
        "\n",
        "Evaluation metrics like accuracy become misleading.\n",
        "\n",
        "Use techniques like:\n",
        "\n",
        "Resampling (oversample minority, undersample majority)\n",
        "\n",
        "Class weights\n",
        "\n",
        "Use precision-recall or F1 score instead of accuracy.\n",
        "\n",
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "It involves optimizing parameters like:\n",
        "\n",
        "λ (regularization strength)\n",
        "\n",
        "Penalty type (L1, L2)\n",
        "\n",
        "Solver\n",
        "\n",
        "Techniques: Grid Search, Random Search, Cross-validation\n",
        "\n",
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "liblinear: Good for small datasets, supports L1 and L2.\n",
        "\n",
        "saga: Supports both L1 and Elastic Net, efficient for large datasets.\n",
        "\n",
        "lbfgs: Good for multiclass problems, fast, supports L2.\n",
        "\n",
        "newton-cg: Efficient for L2-regularization.\n",
        "\n",
        "Recommendation: Use saga for large datasets and lbfgs for multiclass.\n",
        "\n",
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "One-vs-Rest (OvR): Trains a binary classifier for each class.\n",
        "\n",
        "Softmax Regression (Multinomial Logistic Regression): Generalizes logistic regression to handle multiple classes directly.\n",
        "\n",
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "Advantages:\n",
        "\n",
        "Simple and interpretable\n",
        "\n",
        "Fast to train\n",
        "\n",
        "Works well with linearly separable data\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Assumes linearity in log-odds\n",
        "\n",
        "Not good for complex patterns\n",
        "\n",
        "Sensitive to outliers and multicollinearity\n",
        "\n",
        "17. What are some use cases of Logistic Regression?\n",
        "Email spam detection\n",
        "\n",
        "Disease prediction (e.g., diabetes, cancer)\n",
        "\n",
        "Credit scoring\n",
        "\n",
        "Marketing (likelihood of purchase)\n",
        "\n",
        "Customer churn prediction\n",
        "\n",
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "Logistic Regression: For binary classification.\n",
        "\n",
        "Softmax Regression: For multiclass classification, outputs probability distribution over classes using softmax.\n",
        "\n",
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "OvR: Simpler, faster, works well if classes are not highly overlapping.\n",
        "\n",
        "Softmax: More accurate for mutually exclusive classes, better when class dependencies exist.\n",
        "\n",
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "Coefficients represent the change in log-odds of the outcome with a unit change in the predictor.\n",
        "\n",
        "exp⁡(𝑤𝑖)\n",
        "exp(wi): Tells how the odds change (e.g., if\n",
        "exp(𝑤𝑖)=2\n",
        "exp(w i)=2, odds double with 1 unit increase in feature 𝑥𝑖).\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hEuzOd3O1ukO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "#Regression, and prints the model accuracyC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jjMbzrm1za8",
        "outputId": "902cd2cf-c98d-43d3-aefa-20dd59ee0ed9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "#and print the model accuracy\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"L1 Regularized Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R83-k-sP2lZb",
        "outputId": "6eeae7d4-3ac1-495e-f0c5-f6896ec41df1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 Regularized Logistic Regression Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "#LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"L2 Regularized Logistic Regression Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n",
        "print(\"Model Coefficients:\", model.coef_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT2eoL3V2vWZ",
        "outputId": "6aa1b98e-aca1-4308-ebc3-6b17fd747d2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Regularized Logistic Regression Accuracy: 0.956140350877193\n",
            "Model Coefficients: [[ 2.09981182  0.13248576 -0.10346836 -0.00255646 -0.17024348 -0.37984365\n",
            "  -0.69120719 -0.4081069  -0.23506963 -0.02356426 -0.0854046   1.12246945\n",
            "  -0.32575716 -0.06519356 -0.02371113  0.05960156  0.00452206 -0.04277587\n",
            "  -0.04148042  0.01425051  0.96630267 -0.37712622 -0.05858253 -0.02395975\n",
            "  -0.31765956 -1.00443507 -1.57134711 -0.69351401 -0.84095566 -0.09308282]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Elastic Net Logistic Regression Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqIOnoW725_G",
        "outputId": "38a43eab-6476-4c84-ec3e-0727144048a3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Logistic Regression Accuracy: 0.9649122807017544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5.Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "#multi_class='ovr'C\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "X, y = load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Multiclass Logistic Regression (OvR) Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqlO8dDp3Bu8",
        "outputId": "0abbd7a9-83b5-4e82-b53c-b1db2e1981eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass Logistic Regression (OvR) Accuracy: 0.9666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "#Regression. Print the best parameters and accuracy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # Compatible with both L1 and L2\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPHOxxIY3Lbp",
        "outputId": "c08c85e3-6986-483b-f87d-1e4d30af29a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Accuracy: 0.961393363623847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "#average accuracy\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "print(\"Stratified K-Fold Accuracy Scores:\", scores)\n",
        "print(\"Average Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLqNEGuU3gCu",
        "outputId": "1d6c9dfb-d4e9-444e-8b03-611b4e53a08d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stratified K-Fold Accuracy Scores: [0.96666667 0.96111111 0.97214485 0.96657382 0.96935933]\n",
            "Average Accuracy: 0.9671711544413494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "import pandas as pd\n",
        "\n",
        "# Replace with your CSV file path\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Assuming last column is target\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "RqIrMVZp3myz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "#Logistic Regression. Print the best parameters and accuracy\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "param_dist = {\n",
        "    'C': np.logspace(-3, 3, 10),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(LogisticRegression(max_iter=1000), param_distributions=param_dist, n_iter=10, cv=5)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters (Random Search):\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.best_score_)\n",
        "\n"
      ],
      "metadata": {
        "id": "8_L65N_v3yT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "model = OneVsOneClassifier(LogisticRegression(max_iter=1000))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"OvO Multiclass Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n",
        "\n"
      ],
      "metadata": {
        "id": "RlGFM7vC52al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binaryclassification\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(cm).plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "YrqNEfcx58ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "#Recall, and F1-Score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "oU9B6DoX6fvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "#improve model performance\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy with Class Weights:\", accuracy_score(y_test, model.predict(X_test)))\n",
        "\n"
      ],
      "metadata": {
        "id": "OCxy1Eg964Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "#evaluate performanceM\n",
        "import seaborn as sns\n",
        "\n",
        "# Load and preprocess Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "df = df[['sex', 'age', 'fare', 'class', 'survived']].dropna()\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['class'] = df['class'].map({'First': 1, 'Second': 2, 'Third': 3})\n",
        "\n",
        "X = df.drop('survived', axis=1)\n",
        "y = df['survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Titanic Logistic Regression Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n",
        "\n"
      ],
      "metadata": {
        "id": "Lvn5NpW58xUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model.\n",
        "#Evaluate its accuracy and compare results with and without scalingM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy with Scaling:\", accuracy_score(y_test, model.predict(X_test)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GSdpS9CT82i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n",
        "\n"
      ],
      "metadata": {
        "id": "uAr9oCU98_yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy with C=0.5:\", accuracy_score(y_test, model.predict(X_test)))\n",
        "\n"
      ],
      "metadata": {
        "id": "oc2ysJ099GHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18.Write a Python program to train Logistic Regression and identify important features based on model coefficients\n",
        "import numpy as np\n",
        "\n",
        "feature_names = data.feature_names if hasattr(data, \"feature_names\") else X.columns\n",
        "coeffs = model.coef_[0]\n",
        "for name, coef in zip(feature_names, coeffs):\n",
        "    print(f\"{name}: {coef:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UlCWl23W_S8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print(\"Cohen's Kappa Score:\", cohen_kappa_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "p7iCnwSM_Y-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20.Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio:from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HrMo_hXu_cQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n",
        "for solver in ['liblinear', 'saga', 'lbfgs']:\n",
        "    try:\n",
        "        model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "        model.fit(X_train, y_train)\n",
        "        acc = accuracy_score(y_test, model.predict(X_test))\n",
        "        print(f\"Solver = {solver}, Accuracy = {acc:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Solver = {solver} failed: {e}\")\n"
      ],
      "metadata": {
        "id": "CXprATRg_jtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#22.Write a Python program to train Logistic Regression and evaluate its performance using MatthewsCorrelation Coefficient (MCC)\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "print(\"MCC:\", matthews_corrcoef(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "otU9H-ft_lcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact  of feature scaling\n",
        "# Without scaling\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "acc_raw = accuracy_score(y_test, model_raw.predict(X_test))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, random_state=42)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, model_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(f\"Raw Accuracy: {acc_raw:.4f}\")\n",
        "print(f\"Scaled Accuracy: {acc_scaled:.4f}\")\n"
      ],
      "metadata": {
        "id": "sT2yRlvZ_m4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "for c in [0.01, 0.1, 1, 10, 100]:\n",
        "    model = LogisticRegression(C=c, max_iter=1000)\n",
        "    scores = cross_val_score(model, X, y, cv=5)\n",
        "    print(f\"C = {c}, CV Accuracy = {scores.mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "bOPDO2Gg_oaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions\n",
        "import joblib\n",
        "\n",
        "# Train and save\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "joblib.dump(model, 'logistic_model.pkl')\n",
        "\n",
        "# Load and predict\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "y_pred_loaded = loaded_model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy from Loaded Model:\", accuracy_score(y_test, y_pred_loaded))\n"
      ],
      "metadata": {
        "id": "rXXv1Jtq_vhx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}